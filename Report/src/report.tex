%@descr: A template for a paper, a report or a thesis - suitable for modifications
%@author: Maciej Komosi≈Ñski

\documentclass{article} 
\usepackage[english]{babel} 
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{graphicx} %include pdf's (and png's for raster graphics... avoid raster graphics!)
\usepackage{amsmath} %just for \eqref
\usepackage{url}
\usepackage{subcaption} % or \usepackage{subfigure}
\usepackage{float} % for [H] placement specifier
\usepackage{multicol}
\usepackage[pdftex,hyperfootnotes=false,pdfborder={0 0 0}]{hyperref}
%\usepackage{caption_2019-09-01} % after all packages; pdfborder not implemented the same way for every implementation because the specification is imprecise; under miktex you just don't see the frames


\input{_settings}

%\title{}
%\author{}
%\date{}


\begin{document}

\input{_title}





\begin{abstract}
Kolmogorov-Arnold Networks (KANs), proposed as a promising alternative to Multilayer Perceptrons (MLPs),
employ learnable activation functions on edges, represented as spline functions, inspired by the Kolmogorov-Arnold representation theorem.
This paper investigates the influence of spline order and grid size on training, validation, and test accuracy, as well as loss, in KAN networks.
Experimental analyses are conducted on two datasets, MNIST and CIFAR10, to explore the performance of KANs under varying parameters.
\end{abstract}

\section{Introduction}\label{sec:introduction}

KANs\cite{liu2024kan}, introduced as a promising alternative to Multilayer Perceptrons (MLPs),
are inspired by the Kolmogorov-Arnold representation theorem rather than the universal approximation theorem that underpins MLPs.
While MLPs utilize fixed activation functions on nodes, KANs employ learnable activation functions on edges, represented as spline functions.
This unique architecture eliminates traditional linear weight matrices in favor of a flexible and adaptive framework, potentially offering advantages in accuracy and parameter efficiency.

The focus of our research lies in understanding how spline order and grid size impact the training, validation, testing accuracy, and loss in KANs.
Through extensive empirical experiments conducted on two datasets, MNIST and CIFAR10, we aim to elucidate the optimal configuration of KANs for various tasks and datasets.

\section{Experiments}\label{sec:experiments}

\subsection{Experimental Setup}\label{subsec:experimental-setup}

We conduct experiments on the MNIST and CIFAR10 datasets to evaluate the performance of Kolmogorov-Arnold Networks (KANs) with varying spline orders and grid sizes.
The grid size is a critical parameter, dictating the number of control points for the spline functions that define the learnable activation functions on the network's edges.
It modulates the granularity of the spline approximation, allowing for finer or coarser representations of data.
Conversely, the spline order determines the degree of the spline polynomials used in these activation functions, influencing the complexity and flexibility of the model's response to input variations.
\newline
\newline
Our experimental setup includes:
\begin{itemize}
    \item Network Configuration: 4 layers with the number of nodes in the hidden layers set at 128, 64, 32, and 10, respectively.
    \item Optimization: We use the Adam optimizer with a learning rate of 0.001, exponential decay of 0.8, and a batch size of 64.
    \item Duration: The training process spans 10 epochs.
    \item Performance Metrics: We assess the KANs based on training, validation, and test accuracies, as well as loss metrics.
\end{itemize}

\noindent Specific training configurations tested include:
\begin{itemize}
    \item Models with a fixed spline order of 3 across various grid sizes (5, 10, 20, 40, 80, 160, 320, 640, and 1280).
    \item Models with a constant grid size of 5 and varying spline orders from 2 to 12.
\end{itemize}

\noindent To enhance visualization, all figures depicting training accuracy and loss are smoothed using a moving average with a window size of 2 and a scaling factor of 0.9.
\newline

\subsection{MNIST Dataset}\label{subsec:mnist}

The MNIST dataset consists of 60,000 training and 10,000 testing grayscale images of handwritten digits.

\subsubsection{Grid Size Influence}\label{subsubsec:grid-size-influence}

We present the comparison of training and validation loss in Figure~\ref{fig:mnist_loss_grid_size}
and the comparison of training, validation, and test accuracy in Figure~\ref{fig:mnist_accuracy_grid_size} for KANs with varying grid sizes.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_loss_grid_size}
    \caption{Training and validation loss comparison for MNIST.}
    \label{fig:mnist_loss_grid_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_accuracy_grid_size}
    \caption{Training, validation, and test accuracy comparison for MNIST.}
    \label{fig:mnist_accuracy_grid_size}
\end{figure}


\subsubsection{Spline Order Influence}\label{subsubsec:spline-order-influence}

We present the comparison of training and validation loss in Figure~\ref{fig:mnist_loss_spline_order}
and the comparison of training, validation, and test accuracy in Figure~\ref{fig:mnist_accuracy_spline_order} for KANs with varying spline orders.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_loss_spline_order}
    \caption{Training and validation loss comparison for MNIST with varying spline orders.}
    \label{fig:mnist_loss_spline_order}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_accuracy_spline_order}
    \caption{Training, validation, and test accuracy comparison for MNIST with varying spline orders.}
    \label{fig:mnist_accuracy_spline_order}
\end{figure}


\subsection{CIFAR10 Dataset}\label{subsec:cifar10}

The CIFAR10 dataset comprises 50,000 training and 10,000 testing color images of 10 classes.

\subsubsection{Grid Size Influence}\label{subsubsec:grid-size-influence-cifar10}

Similarly to the MNIST dataset, we present the comparison of training and validation loss in Figure~\ref{fig:cifar10_loss_grid_size}
and the comparison of training, validation, and test accuracy in Figure~\ref{fig:cifar10_accuracy_grid_size} for KANs with varying grid sizes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_loss_grid_size}
    \caption{Training and validation loss comparison for CIFAR10.}
    \label{fig:cifar10_loss_grid_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_accuracy_grid_size}
    \caption{Training, validation, and test accuracy comparison for CIFAR10.}
    \label{fig:cifar10_accuracy_grid_size}
\end{figure}


\subsubsection{Spline Order Influence}\label{subsubsec:spline-order-influence-cifar10}

Similarly to the MNIST dataset, we also present the comparison of training and validation loss in Figure~\ref{fig:cifar10_loss_spline_order}
and the comparison of training, validation, and test accuracy in Figure~\ref{fig:cifar10_accuracy_spline_order} for KANs with varying spline orders.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_loss_spline_order}
    \caption{Training and validation loss comparison for CIFAR10 with varying spline orders.}
    \label{fig:cifar10_loss_spline_order}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_accuracy_spline_order}
    \caption{Training, validation, and test accuracy comparison for CIFAR10 with varying spline orders.}
    \label{fig:cifar10_accuracy_spline_order}
\end{figure}


\section{Conclusions}\label{sec:conclusions}
Our experiments with Kolmogorov-Arnold Networks have provided clear insights into the impacts of grid size and spline order on model performance.
Contrary to expectations, we discovered that smaller grid sizes significantly enhance both training and validation performance across the datasets studied.
Interestingly, larger grid sizes, which provide more control points, were more disruptive than beneficial,
likely due to the overwhelming number of potential adjustments preventing effective model tuning.

In terms of spline orders, the increased complexity of the spline functions initially did not improve performance during the training phase and sometimes even underperformed compared to models with fewer spline orders.
However, during the validation phase, higher spline orders proved advantageous, suggesting that they are better suited for capturing more nuanced features that are not apparent during training.
Conversely, lower spline orders, while simplifying the model, tended to overlook subtle data intricacies, which hampered their ability to generalize effectively.

These findings underscore the delicate balance between model complexity and performance efficacy.
They highlight the importance of meticulously selecting grid size and spline order to align with the specific demands and limitations of each task.
Looking forward, the most promising strategy appears to be the development of adaptive mechanisms that dynamically adjust these parameters in response to evolving data characteristics during training.
Such adaptive strategies could significantly enhance both the efficiency and the accuracy of KANs, broadening their applicability across diverse applications.


\clearpage % let LaTeX put pending figures right here -- this command "releases" the accumulated content, which is useful if you have placed a lot of images, much more than text -- so they do not appear at the end of the document.


%%%%%%%%%%%%%%%% references %%%%%%%%%%%%%%%%

\bibliography{biblio}
\bibliographystyle{plain}


\end{document}
