%@descr: A template for a paper, a report or a thesis - suitable for modifications
%@author: Maciej Komosi≈Ñski

\documentclass{article} 
\usepackage[english]{babel} 
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{graphicx} %include pdf's (and png's for raster graphics... avoid raster graphics!)
\usepackage{amsmath} %just for \eqref
\usepackage{url}
\usepackage{subcaption} % or \usepackage{subfigure}
\usepackage{float} % for [H] placement specifier
\usepackage{multicol}
\usepackage[pdftex,hyperfootnotes=false,pdfborder={0 0 0}]{hyperref}
%\usepackage{caption_2019-09-01} % after all packages; pdfborder not implemented the same way for every implementation because the specification is imprecise; under miktex you just don't see the frames


\input{_settings}

%\title{}
%\author{}
%\date{}


\begin{document}

\input{_title}





\begin{abstract}
Kolmogorov-Arnold Networks (KANs), proposed as a promising alternative to Multilayer Perceptrons (MLPs),
employ learnable activation functions on edges, represented as spline functions, inspired by the Kolmogorov-Arnold representation theorem.
This paper investigates the influence of spline order and grid size on training, validation, and test accuracy, as well as loss, in KAN networks.
Experimental analyses are conducted on two datasets, MNIST and CIFAR10, to explore the performance of KANs under varying parameters.
\end{abstract}

\section{Introduction}\label{sec:introduction}

KANs\cite{liu2024kan}, introduced as a promising alternative to Multilayer Perceptrons (MLPs),
are inspired by the Kolmogorov-Arnold representation theorem rather than the universal approximation theorem that underpins MLPs.
While MLPs utilize fixed activation functions on nodes, KANs employ learnable activation functions on edges, represented as spline functions.
This unique architecture eliminates traditional linear weight matrices in favor of a flexible and adaptive framework, potentially offering advantages in accuracy and parameter efficiency.

The focus of our research lies in understanding how spline order and grid size impact the training, validation, testing accuracy, and loss in KANs.
Through extensive empirical experiments conducted on two datasets, MNIST and CIFAR10, we aim to elucidate the optimal configuration of KANs for various tasks and datasets.

\section{Experiments}\label{sec:experiments}

\subsection{Experimental Setup}\label{subsec:experimental-setup}

We conduct experiments on the MNIST and CIFAR10 datasets to evaluate the performance of Kolmogorov-Arnold Networks (KANs) with varying spline orders and grid sizes.
The grid size is a critical parameter, dictating the number of control points for the spline functions that define the learnable activation functions on the network's edges.
It modulates the granularity of the spline approximation, allowing for finer or coarser representations of data.
Conversely, the spline order determines the degree of the spline polynomials used in these activation functions, influencing the complexity and flexibility of the model's response to input variations.
\newline
\newline
Our experimental setup includes:
\begin{itemize}
    \item Network Configuration: 4 layers with the number of nodes in the hidden layers set at 128, 64, 32, and 10, respectively.
    \item Optimization: We use the Adam optimizer with a learning rate of 0.001, exponential decay of 0.8, and a batch size of 64.
    \item Duration: The training process spans 10 epochs.
    \item Performance Metrics: We assess the KANs based on training, validation, and test accuracies, as well as loss metrics.
\end{itemize}

\noindent Specific training configurations tested include:
\begin{itemize}
    \item Models with a fixed spline order of 3 across various grid sizes (5, 10, 20, 40, 80, 160, 320, 640, and 1280).
    \item Models with a constant grid size of 5 and varying spline orders from 2 to 12.
\end{itemize}

\noindent To enhance visualization, all figures depicting training accuracy and loss are smoothed using a moving average with a window size of 2 and a scaling factor of 0.9.
\newline

\subsection{MNIST Dataset}\label{subsec:mnist}

The MNIST dataset consists of 60,000 training and 10,000 testing grayscale images of handwritten digits.

\subsubsection{Grid Size Influence}\label{subsubsec:grid-size-influence}

We present the comparison of training and validation loss in Figure~\ref{fig:mnist_loss_grid_size}
and the comparison of training and validation accuracy in Figure~\ref{fig:mnist_accuracy_grid_size} for KANs with varying grid sizes.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_loss_grid_size}
    \caption{Training and validation loss comparison for MNIST.}
    \label{fig:mnist_loss_grid_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_accuracy_grid_size}
    \caption{Training and validation accuracy comparison for MNIST.}
    \label{fig:mnist_accuracy_grid_size}
\end{figure}


The test accuracy results for KANs with varying grid sizes are presented in Table~\ref{tab:test_accuracy_grid_size}.

\begin{table}[H]
    \centering
    \caption{Test accuracy for KANs with varying grid sizes on the MNIST dataset.}
    \label{tab:test_accuracy_grid_size}
    \begin{tabular}{|c|c|}
        \hline
        Grid Size & Test Accuracy \\
        \hline
        \textbf{5}    & \textbf{0.551851} \\
        10   & 0.521994 \\
        20   & 0.504479 \\
        40   & 0.496019 \\
        80   & 0.479001 \\
        160  & 0.446855 \\
        320  & 0.438296 \\
        640  & 0.375697 \\
        2560 & 0.328921 \\
        \hline
    \end{tabular}
\end{table}



\subsubsection{Spline Order Influence}\label{subsubsec:spline-order-influence}

We present the comparison of training and validation loss in Figure~\ref{fig:mnist_loss_spline_order}
and the comparison of training and validation accuracy in Figure~\ref{fig:mnist_accuracy_spline_order} for KANs with varying spline orders.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_loss_spline_order}
    \caption{Training and validation loss comparison for MNIST with varying spline orders.}
    \label{fig:mnist_loss_spline_order}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/mnist_accuracy_spline_order}
    \caption{Training and validation accuracy comparison for MNIST with varying spline orders.}
    \label{fig:mnist_accuracy_spline_order}
\end{figure}


The test accuracy results for KANs with varying spline orders are presented in decreasing by accuracy order in Table~\ref{tab:test_accuracy_spline_order}.

\begin{table}[H]
    \centering
    \caption{Test accuracy for KANs with varying spline orders on the MNIST dataset.}
    \label{tab:test_accuracy_spline_order}
    \begin{tabular}{|c|c|}
        \hline
        Spline Order & Test Accuracy \\
        \hline
        \textbf{9}           & \textbf{0.975870} \\
        8           & 0.975376 \\
        11          & 0.974782 \\
        4           & 0.974288 \\
        7           & 0.974189 \\
        6           & 0.974189 \\
        5           & 0.973695 \\
        12          & 0.973695 \\
        3           & 0.973596 \\
        10          & 0.973101 \\
        2           & 0.970134 \\
        \hline
    \end{tabular}
\end{table}



\subsection{CIFAR10 Dataset}\label{subsec:cifar10}

The CIFAR10 dataset comprises 50,000 training and 10,000 testing color images of 10 classes.

\subsubsection{Grid Size Influence}\label{subsubsec:grid-size-influence-cifar10}

Similarly to the MNIST dataset, we present the comparison of training and validation loss in Figure~\ref{fig:cifar10_loss_grid_size}
and the comparison of training and validation accuracy in Figure~\ref{fig:cifar10_accuracy_grid_size} for KANs with varying grid sizes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_loss_grid_size}
    \caption{Training and validation loss comparison for CIFAR10.}
    \label{fig:cifar10_loss_grid_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_accuracy_grid_size}
    \caption{Training and validation accuracy comparison for CIFAR10.}
    \label{fig:cifar10_accuracy_grid_size}
\end{figure}


The test accuracy results for KANs with varying grid sizes on the CIFAR10 dataset are presented in Table~\ref{tab:test_accuracy_grid_size_cifar10}.

\begin{table}[H]
    \centering
    \caption{Test accuracy for KANs with varying grid sizes on the CIFAR10 dataset.}
    \label{tab:test_accuracy_grid_size_cifar10}
    \begin{tabular}{|c|c|}
        \hline
        Grid Size & Test Accuracy \\
        \hline
        \textbf{5}    & \textbf{0.551851} \\
        10   & 0.521994 \\
        20   & 0.504479 \\
        40   & 0.496019 \\
        80   & 0.479001 \\
        160  & 0.446855 \\
        320  & 0.438296 \\
        640  & 0.375697 \\
        2560 & 0.328921 \\
        \hline
    \end{tabular}
\end{table}


\subsubsection{Spline Order Influence}\label{subsubsec:spline-order-influence-cifar10}

Similarly to the MNIST dataset, we also present the comparison of training and validation loss in Figure~\ref{fig:cifar10_loss_spline_order}
and the comparison of training and validation accuracy in Figure~\ref{fig:cifar10_accuracy_spline_order} for KANs with varying spline orders.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_loss_spline_order}
    \caption{Training and validation loss comparison for CIFAR10 with varying spline orders.}
    \label{fig:cifar10_loss_spline_order}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{pics/cifar10_accuracy_spline_order}
    \caption{Training and validation accuracy comparison for CIFAR10 with varying spline orders.}
    \label{fig:cifar10_accuracy_spline_order}
\end{figure}


The test accuracy results for KANs with varying spline orders on the CIFAR10 dataset are presented in Table~\ref{tab:test_accuracy_spline_order_cifar10}.

\begin{table}[H]
    \centering
    \caption{Test accuracy for KANs with varying spline orders on the CIFAR10 dataset.}
    \label{tab:test_accuracy_spline_order_cifar10}
    \begin{tabular}{|c|c|}
        \hline
        Spline Order & Test Accuracy \\
        \hline
        \textbf{9}  & \textbf{0.552647} \\
        10 & 0.550557 \\
        6  & 0.550358 \\
        11 & 0.549264 \\
        12 & 0.548666 \\
        4  & 0.547870 \\
        8  & 0.547174 \\
        7  & 0.546676 \\
        5  & 0.541103 \\
        3  & 0.551851 \\
        2  & 0.539411 \\
        \hline
    \end{tabular}
\end{table}


\section{Conclusions}\label{sec:conclusions}

\setlength{\parindent}{0pt} % no indentation
\setlength{\parskip}{\baselineskip} % blank line between paragraphs

Our experiments with Kolmogorov-Arnold Networks have provided clear insights into the impacts of grid size and spline order on model performance.
Contrary to expectations, we discovered that smaller grid sizes significantly enhance both training and validation performance across the datasets studied.
Interestingly, larger grid sizes, which provide more control points, were more disruptive than beneficial,
likely due to the overwhelming number of potential adjustments preventing effective model tuning.

In our investigation, we found that the spline order has very little effect on the performance of KANs for both the MNIST and CIFAR10 datasets.
This suggests that the increased complexity of the spline functions, indicated by higher spline orders, does not significantly contribute to the accuracy or loss metrics during the training, validation, or testing phases.
These findings indicate that the choice of spline order can be considered a minor factor in the configuration of KANs.
Practitioners may opt for lower spline orders to simplify the model without sacrificing performance, as higher spline orders do not provide noticeable benefits in capturing more nuanced features or improving generalization.

Furthermore, the number of model parameters in KANs is in total \(O(N^2 L (G + k)) \sim O(N^2 LG)\), where \(L\) is the depth, \(N\) is the width of the widest layer, \(G\) is the grid size, and \(k\) is the spline order.
Since \(G\) is usually much larger than \(k\), as observed in our experiments, the parameter complexity simplifies to \(O(N^2 LG)\).
This explains why changing the spline order by one has less influence on model performance compared to changing the grid size by a large value.
The grid size, being a more dominant factor in the parameter count, significantly impacts the model's ability to tune effectively, thus affecting overall performance more profoundly.

These findings underscore the delicate balance between model complexity and performance efficacy.
They highlight the importance of meticulously selecting grid size and spline order to align with the specific demands and limitations of each task.
Looking forward, the most promising strategy appears to be the development of adaptive mechanisms that dynamically adjust these parameters in response to evolving data characteristics during training.
Such adaptive strategies could significantly enhance both the efficiency and the accuracy of KANs, broadening their applicability across diverse applications.


\clearpage % let LaTeX put pending figures right here -- this command "releases" the accumulated content, which is useful if you have placed a lot of images, much more than text -- so they do not appear at the end of the document.


%%%%%%%%%%%%%%%% references %%%%%%%%%%%%%%%%

\bibliography{biblio}
\bibliographystyle{plain}


\end{document}
